\chapter{Methodology}
\label{chap:methodology}

In this chapter, we will cover the methodology used in this work. More details on the pipeline execution will be provided in Section \ref{sec:backend}.

\section{Prediction Using the CryptoBench Model, ESM-2}
\label{sec:prediction}

The initial step in our methodology involves utilizing the CryptoBench \cite{vskrhak2025cryptobench} model to generate residue-level predictions for protein sequences. It is important to note that this model works solely with sequence information, without incorporating any structural (tertiary or quaternary) data. Specifically, we employ the tiny variant of CryptoBench\footnote{Available at \url{https://github.com/skrhakv/TinyCryptobench}}, which is trained on a reduced version of the fine-tuned ESM-2 embeddings \cite{lin2022language} with 650 million parameters, as opposed to the full 3 billion. This choice is motivated by the observation that prediction accuracy remains comparable, while computational requirements are significantly reduced, enabling the pipeline to run efficiently on standard personal computers without the need for a GPU.

CryptoBench was created by filtering the AHoJ-DB \cite{feidakis2024ahoj} dataset, which contains a few million binding pockets. The filtering steps involved resolution filtering, removing redundant structures, geometric quality assurance, crypticity constraints, ligand filtering and sequence clustering. The final dataset consists of 1107 apo structures with 1361 cryptic binding sites. This dataset is used to train the model that has been benchmarked against PocketMiner \cite{meller2023predicting} and P2Rank \cite{krivak2018p2rank} and has shown better performance with the apo structures.

\sloppy
To begin with the prediction, the necessary dependencies for the CryptoBench model must be installed. Once the Python environment is set up, the fine-tuned ESM-2 model (\lstinline!facebook/esm2_t33_650M_UR50D!) is loaded, followed by the corresponding weight file from the Tiny-CryptoBench repository. The protein sequence is then tokenized using the ESM-2 tokenizer, segmented into chunks of 1022 tokens (1024 minus two special tokens), and processed by the CryptoBench model to obtain predictions. These predictions are subsequently concatenated to produce a single prediction vector representing the entire sequence.

\sloppy
The implementation for this procedure is provided in the \lstinline!backend/prediction/compute_score.py! file. The code is intended for use with individual protein sequences, as it requires parsing and splitting the sequence by chain to ensure accurate predictions.

This code is run asynchronously using Celery workers, allowing efficient processing of multiple sequences in parallel. Moreover, the implementation supports both GPU and CPU execution.

After the predictions are generated, the next step is to look for potential clusters of high-scoring residues and create the corresponding binding site candidates, as described in Section~\ref{sec:clustering}.

\section{Clustering and Smoothing}
\label{sec:clustering}

The next step in our methodology is to take the residue-level predictions generated by the CryptoBench model and cluster the high-scoring residues to form potential binding site candidates. This is crucial for the visualization as coloring the clusters instead of just individual residues increases the interpretability of the results. The clustering process is based on the 3D coordinates of individual residues, which are obtained from the given structure file.

The clustering process is implemented in the \lstinline!backend/clustering! dictionary. There are many algorithms available for clustering, namely DBSCAN \cite{schubert2017dbscan}, single-linkage clustering, hierarchical clustering \cite{jarman2020hierarchical}, and many more. In our case, we opted for the DBSCAN algorithm, which is a density-based clustering algorithm that groups together points that are closely packed together. This approach is particularly suitable for our use case, as it allows us to identify clusters of high-scoring residues without requiring a predefined number of clusters, and allows the specification of minimum residue distance and minimum number of residues in a cluster.

Our method relies on three parameters to control the clustering process:
\begin{itemize}
    \item \textbf{EPS ($\epsilon$)} - This parameter defines the maximum distance between two residues for them to be considered as part of the same cluster. In our case, we opted for a value of \textbf{5.0 \AA}, which is a reasonable distance for residues when considering their spatial proximity in a protein structure.
    \item \textbf{Minimum cluster size} - This parameter defines the minimum number of residues required to form a cluster. We set this value to \textbf{3}, as 2 residues are not sufficient to form a binding site candidate.
    \item \textbf{Minimum (CryptoBench) prediction score} - This parameter defines the minimum score a residue must have to be marked as a high-scoring residue. We opted for a value of \textbf{0.7}, which covers most cryptic binding sites in the CryptoBench dataset, while also not marking the entire protein as a binding site candidate.
\end{itemize}

The values for these parameters were chosen arbitrarily based on previous experience and benchmarks (covered in this section below). However, these parameters can be adjusted to suit the specific needs. In the implementation, the \textbf{scikit-learn} implementation of the DBSCAN algorithm is used. The clustering works with two method parameters, the 3D coordinates of the residues and the CryptoBench prediction scores.

After the clustering is performed, it seems like the task is done, but there are still some steps to perform. The first problem is to know if the method performs well, i.e. if the clusters correspond to actual binding sites. To address this, we have created a benchmarking notebook that allows us to visualize interesting metrics. We will compare the clusters with the known binding sites from the CryptoBench test set, which contains a few hundred cryptic binding sites. The benchmarking notebooks and other source codes are available in the \lstinline!cryptoshow-benchmark! directory. Note that:

\begin{itemize}
    \item The benchmarking is performed on the CryptoBench test set, not the tiny variant used in the pipeline. This might lead to slightly different results, but the overall performance should be similar.
    \item Multi-chain pockets are omitted from the benchmarking, as the process would be much more complicated and the prediction would require complex approaches.
    \item Many cryptic binding sites in the test set are similar to each other, which can lead to heavily inflated results. To address this, we have used the \textbf{Szymkiewicz–Simpson coefficient (overlap)}, which is a measure of similarity between two sets $A, B$. The coefficient is defined in Equation~\ref{eq:szymkiewicz-simpson}.
    \begin{equation}
        \text{Szymkiewicz–Simpson}(A, B) = \frac{|A \cap B|}{\min(|A|, |B|)}
        \label{eq:szymkiewicz-simpson}
    \end{equation}
    After computing the overlap between each pair of cryptic binding sites in one structure, we keep only the CBSs with the overlaps under 0.5.
\end{itemize}

The benchmarking follows the same procedure as the CryptoShow pipeline - first, the predictions are generated for each of the test set structures, then the clustering is performed with the same parameters as in the pipeline. After creating the clusters, we compute the following metrics for each cluster and compare them with the known binding sites:

\begin{itemize}
    \item \textbf{Distance between the cluster center and the CBS center} - This metric is computed as the distance between the cluster center and the center of the cryptic binding site. Both centers are computed as the average of the 3D coordinates of the residues in the cluster and the CBS, respectively. The distance is computed in \AA.
    \item \textbf{Percentage of CBS residues covered by the cluster} - This metric is computed as the percentage of residues in the CBS that are also part of the cluster.
    \item \textbf{Dice coefficient} - This metric is computed as the ratio of the number of residues in the intersection of the cluster and the CBS to the average number of residues in both sets $A, B$. It is defined in Equation~\ref{eq:dice-coefficient}.
    \begin{equation}
        \text{Dice}(A, B) = \frac{2 |A \cap B|}{|A| + |B|}
        \label{eq:dice-coefficient}
    \end{equation}
\end{itemize}

After computing the metrics for all clusters and one specific CBS, the best-matching cluster is selected based on the top-scoring metric. After doing this for all CBSs and structures, we can see the overall performance of the clustering method. The results are visualized in Figure \ref{fig:clustering-benchmark}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/non-smoothened-1.png}
    \caption{Clustering benchmark results before pocket smoothing. The left plot shows the distance between the cluster center and the CBS center, and the right plot shows percentage of residues covered by the found clusters.}
    \label{fig:clustering-benchmark}
\end{figure}

The results show that the method performs rather poorly, with the average distance between the cluster center and the CBS center being around 10 \AA, and the percentage of CBS residues covered by the cluster being around 25\%. This might not be a bad result as one missing residue in the binding site can lead to a significant increase in the distance, but the percentage of residues covered is rather low. This is caused by the fact that the clustering is performed only on the high-scoring residues. But, some of the CBSs have residues with low scores (supported by Figure \xxx{TODO}), which are still part of the binding site.

What to do next? The approach we decided to try together in cooperation with Vít Škrhák (thanks! \xxx{TODO: how to credit properly here when there is no paper, should I add an acknoweledgements section?}) was to train another machine learning model that would take the protein sequence, predicted cluster by the clustering method (together with the sequence information), ESM-2 embedding computed earlier and then information about the residue distances taken from the structure file. All together, this \textbf{smoothing} model will be trained on a modified version of the CryptoBench dataset - by this we mean that the cryptic binding sites from the CryptoBench dataset will be taken as a ground truth.

First, we have to prepare the dataset for training. Before the preparation, we need to make sure we have the pre-computed embeddings from the first step saved somewhere. Then, we need to compute the distance matrix for the coordinates - this is done simply by using Equation~\ref{eq:distance-matrix},

\begin{equation}
D_{ij} = \left\| \mathbf{c}_i - \mathbf{c}_j \right\|_2 = \sqrt{ \sum_{k=1}^3 (c_{ik} - c_{jk})^2 }
\label{eq:distance-matrix}
\end{equation}

where $D_{ij}$ is the distance between residues $i$ and $j$, and $\mathbf{c}_i, \mathbf{c}_j$ are their 3D coordinates.

After computing the distance matrices, we need to create both positive and negative examples for the training. First, we get the residues that are close to the actual binding residues by checking if the distance between a residue and any binding residue is less than a chosen threshold (the positive distance threshold, chosen as 15 \AA). For each such residue, we aggregate its embedding with the mean embedding of its close binding neighbors to form a feature vector, which is labeled as a positive example.

For negative examples, we select residues that are very close (within a smaller negative distance threshold of 10 \AA) to binding residues but are not themselves binding residues. These are labeled as negative examples. This approach ensures that the model learns to distinguish between true binding residues and those that are spatially close but not part of the binding site.

The vectors are later converted into torch tensors. We do this for both training and test data and create the two datasets that we will use during the training.

For the actual training, we first compute the class weights and load the data. Then, we train a neural network with the same architecture as in Section \ref{sec:prediction} with different thresholds. The model is then tested and several metrics such as F1 score, ROC/AUC and accuracy are computed. \xxx{TODO: add a figure of ROC with some scores, explain the metrics with formulas?} After this computation, we save the model to use it later in the pipeline.

In the smoothing step, we load the model, specify the threshold (we select the best threshold of 0.7 based on F1 score of the test set results) and prepare the data for the relevant structure/sequence. After the inference, we receive the smoothened pocket, which might include other residues that should have been a part of the potential cryptic binding site, even though their score is not above the threshold for the high-scoring residues.

The results seem more promising. As shown in Figure \ref{fig:clustering-benchmark-smoothened}, we achieve an average distance of the pocket centers of 5.12 \AA (compared to 9.26 \AA without the smoothing), average percentage of residues covered of 77.8 \% (opposed to 25.5 \%), and average Dice coefficient of 0.51 (opposed to 0.32). This proves that the smoothing method drastically improves the prediction. \xxx{TODO: add figures for the second results?} Although the predictions are not perfect, most of the predicted pockets have the distance of the pocket centers in the range of 0 to 5 \AA, which is a good result.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/smoothened-1.png}
    \caption{Clustering benchmark results after pocket smoothing. The left plot shows the distance between the cluster center and the CBS center, and the right plot shows percentage of residues covered by the found clusters.}
    \label{fig:clustering-benchmark-smoothened}
\end{figure}

All source codes related to the smoothing method are available in the \lstinline!cryptoshow-benchmark/smoothing! directory, including the training code, train and test sets, inference code and code for distance matrix computation.

\xxx{TODO: maybe add the architecture of the neural network to 2.1? i.e. the layer description, ReLU etc.}
\xxx{TODO: add citations for the chapter 2}
\xxx{TODO: potential drawbacks}
\xxx{TODO: reformat/rephrase the text in 2.2}

\section{Trajectory Animation}
\label{sec:trajectory}

\xxx{TODO}
